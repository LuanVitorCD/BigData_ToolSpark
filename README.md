# Big Data com Apache Spark ğŸš€

> Pipeline bÃ¡sico em PySpark para ingestÃ£o, processamento, armazenamento e visualizaÃ§Ã£o de grandes volumes de dados.

---

## ğŸ“‹ SumÃ¡rio

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Funcionalidades](#funcionalidades)
3. [PrÃ©-requisitos](#prÃ©-requisitos)
4. [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
5. [ExecuÃ§Ã£o](#execuÃ§Ã£o)
6. [Estrutura do Projeto](#estrutura-do-projeto)
7. [CustomizaÃ§Ã£o](#customizaÃ§Ã£o)
8. [ContribuiÃ§Ã£o](#contribuiÃ§Ã£o)
9. [LicenÃ§a](#licenÃ§a)

---

## ğŸ§ VisÃ£o Geral
Este repositÃ³rio demonstra um fluxo de trabalho completo em **PySpark**:

- **IngestÃ£o** de dados CSV
- **Processamento**: filtragem, agregaÃ§Ã£o e transformaÃ§Ãµes
- **Armazenamento** em **Parquet**
- **VisualizaÃ§Ã£o** dos resultados com **Pandas** e **Matplotlib**

Ideal para quem deseja entender como aplicar o Spark em cenÃ¡rios reais de Big Data.

---

## âœ¨ Funcionalidades

- Leitura de arquivos CSV com cabeÃ§alho
- Limpeza de dados (remoÃ§Ã£o de valores nulos)
- AgregaÃ§Ã£o por categoria
- ExportaÃ§Ã£o de resultados em formato Parquet
- GeraÃ§Ã£o de grÃ¡ficos de barra a partir de DataFrame Pandas

---

## ğŸ¯ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter instalado:

- **Python 3.7+**
- **Java 8+** (necessÃ¡rio para o Spark)
- **Pip** (gerenciador de pacotes Python)

Pacotes Python (via `requirements.txt`):

- `pyspark`
- `pandas`
- `matplotlib`

---

## ğŸš€ InstalaÃ§Ã£o

1. **Clone este repositÃ³rio**
   ```bash
   git clone https://github.com/SEU_USUARIO/bigdata-spark-project.git
   cd bigdata-spark-project
````

2. **Crie e ative um ambiente virtual (opcional)**

   ```bash
   python -m venv venv
   source venv/bin/activate   # Linux/macOS
   venv\Scripts\activate      # Windows
   ```

3. **Instale as dependÃªncias**

   ```bash
   pip install -r requirements.txt
   ```

---

## â–¶ï¸ ExecuÃ§Ã£o

Execute o script principal:

```bash
python pyspark_project.py
```

* Os arquivos Parquet serÃ£o gravados em `data/processed/`.
* Um grÃ¡fico de barras serÃ¡ exibido em tela mostrando a soma de valores por categoria.

---

## ğŸ—‚ï¸ Estrutura do Projeto

```plaintext
bigdata-spark-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ exemplo.csv        # Dados de entrada
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ total_por_categoria.parquet  # SaÃ­da do pipeline
â”œâ”€â”€ pyspark_project.py         # Script principal em PySpark
â”œâ”€â”€ requirements.txt           # DependÃªncias do projeto
â””â”€â”€ README.md                  # DocumentaÃ§Ã£o (vocÃª estÃ¡ aqui)
```

---

## âš™ï¸ CustomizaÃ§Ã£o

* **Caminhos de dados**: altere `data/raw/exemplo.csv` e `data/processed/` no script.
* **TransformaÃ§Ãµes**: adicione novas operaÃ§Ãµes no **DataFrame** PySpark.
* **VisualizaÃ§Ã£o**: modifique o cÃ³digo de plot em Matplotlib conforme sua necessidade.
* **MLlib**: acrescente etapas de machine learning utilizando `pyspark.ml`.

---

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas!

1. Fork este repositÃ³rio
2. Crie uma branch: `git checkout -b feature/nova-funcionalidade`
3. Commit suas mudanÃ§as: `git commit -m 'Adiciona nova funcionalidade'`
4. Push para a branch: `git push origin feature/nova-funcionalidade`
5. Abra um Pull Request

---

## ğŸ“ LicenÃ§a

DistribuÃ­do sob a licenÃ§a MIT. Veja [LICENSE](LICENSE) para mais detalhes.

```
```
